# Kubernetes Level 2 (Tasks 2.1-2.11) 

For all tasks: `Note:` The `kubectl` utility on `jump_host` has been configured to work with the kubernetes cluster.

## 2.1 Kubernetes Shared Volumes

We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.

1. Create a pod named `volume-share-devops`.
2. For the first container, use image `ubuntu` with `latest` tag only and remember to mention the tag i.e `ubuntu:latest`, container should be named as `volume-container-devops-1`, and run a `sleep` command for it so that it remains in running state. Volume `volume-share` should be mounted at path `/tmp/blog`.
3. For the second container, use image `ubuntu` with the `latest` tag only and remember to mention the tag i.e `ubuntu:latest`, container should be named as `volume-container-devops-2`, and again run a `sleep` command for it so that it remains in running state. Volume `volume-share` should be mounted at path `/tmp/cluster`.
4. Volume name should be `volume-share` of type `emptyDir`.
5. After creating the pod, exec into the first container i.e `volume-container-devops-1`, and just for testing create a file `blog.txt` with any content under the mounted path of first container i.e `/tmp/blog`.
6. The file `blog.txt` should be present under the mounted path `/tmp/cluster` on the second container `volume-container-devops-2` as well, since they are using a shared volume.

---

Check the existing resources:
```bash
$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   88m
```

Create the pod.yaml and apply, then verify:
```bash
$ cat pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: volume-share-devops
spec:
  containers:
  - name: volume-container-devops-1
    image: ubuntu:latest
    command: ["sleep", "infinity"]
    volumeMounts:
    - name: volume-share
      mountPath: /tmp/blog
  - name: volume-container-devops-2
    image: ubuntu:latest
    command: ["sleep", "infinity"]
    volumeMounts:
    - name: volume-share
      mountPath: /tmp/cluster
  volumes:
  - name: volume-share
    emptyDir: {}

$ kubectl apply -f pod.yaml 
pod/volume-share-devops created

$ kubectl get pods
NAME                  READY   STATUS    RESTARTS   AGE
volume-share-devops   2/2     Running   0          35s
```

Execute into the first container and create a file `blog.txt` with any content under the mounted path of first container i.e `/tmp/blog`.
```bash
$ kubectl exec -it volume-share-devops -c volume-container-devops-1 -- sh
# cd tmp/blog
# touch blog.txt
# ls -la
total 8
drwxrwxrwx 2 root root 4096 Feb  9 13:34 .
drwxrwxrwt 1 root root 4096 Feb  9 13:29 ..
-rw-r--r-- 1 root root    0 Feb  9 13:34 blog.txt
```

Execute into the second container and verify that they share the volume:
```bash
$ kubectl exec -it volume-share-devops -c volume-container-devops-2 -- ls -la tmp/cluster
total 8
drwxrwxrwx 2 root root 4096 Feb  9 13:34 .
drwxrwxrwt 1 root root 4096 Feb  9 13:29 ..
-rw-r--r-- 1 root root    0 Feb  9 13:34 blog.txt
```
✅

## 2.2 Kubernetes Sidecar Containers

We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

1. Create a pod named `webserver`.
2. Create an `emptyDir` volume `shared-logs`.
3. Create two containers from `nginx` and `ubuntu` images with `latest` tag only and remember to mention tag i.e `nginx:latest`, nginx container name should be `nginx-container` and ubuntu container name should be `sidecar-container` on webserver pod.
4. Add command on sidecar-container `"sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"`
5. Mount the volume `shared-logs` on both containers at location `/var/log/nginx`, all containers should be up and running.

---

Check the existing resources:
```bash
$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   81m
```

Create the webserver-pod.yaml and apply, then verify:
```bash
$ cat webserver-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  - name: sidecar-container
    image: ubuntu:latest
    command: ["sh", "-c", "while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"]
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  volumes:
  - name: shared-logs
    emptyDir: {}

$ kubectl apply -f webserver-pod.yaml 
pod/webserver created

$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
webserver   2/2     Running   0          96s
```

Verify that the container started successfully by checking Events:
```bash
$ kubectl describe pod webserver | grep Events -A 15
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m13s  default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
  Normal  Pulling    3m12s  kubelet            Pulling image "nginx:latest"
  Normal  Pulled     3m2s   kubelet            Successfully pulled image "nginx:latest" in 9.556998572s (9.55701751s including waiting)
  Normal  Created    3m2s   kubelet            Created container nginx-container
  Normal  Started    3m2s   kubelet            Started container nginx-container
  Normal  Pulling    3m2s   kubelet            Pulling image "ubuntu:latest"
  Normal  Pulled     2m58s  kubelet            Successfully pulled image "ubuntu:latest" in 4.468274567s (4.468283753s including waiting)
  Normal  Created    2m58s  kubelet            Created container sidecar-container
  Normal  Started    2m57s  kubelet            Started container sidecar-container
```
✅

## 2.3 Deploy Nginx Web Server on Kubernetes Cluster

Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:

1. Create a deployment using `nginx` image with `latest` tag only and remember to mention the tag i.e `nginx:latest`. Name it as `nginx-deployment`. The container should be named as `nginx-container`, also make sure replica counts are `3`.
2. Create a `NodePort` type service named `nginx-service`. The nodePort should be `30011`.

---

Check the existing resources:
```bash
$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   30m
```

Create the webserver-pod.yaml and apply:
```bash
$ cat >nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest
        ports:
        - containerPort: 80

$ kubectl apply -f nginx-deployment.yaml
deployment.apps/nginx-deployment created
```

Create the webserver-pod.yaml and apply:
```bash
$ cat >nginx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30011

$ kubectl apply -f nginx-service.yaml
service/nginx-service created
```

Check the resources:
```bash
$ kubectl get all
NAME                                    READY   STATUS    RESTARTS   AGE
pod/nginx-deployment-5b58668cfc-4ntz8   1/1     Running   0          111s
pod/nginx-deployment-5b58668cfc-j5wql   1/1     Running   0          111s
pod/nginx-deployment-5b58668cfc-vklxw   1/1     Running   0          111s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP        33m
service/nginx-service   NodePort    10.96.93.184   <none>        80:30011/TCP   60s

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-deployment   3/3     3            3           111s

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-deployment-5b58668cfc   3         3         3       111s
```

![3](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/f4868c2a-1eee-4da9-87a5-624558fb968d)
✅

## 2.4 Print Environment Variables

The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.

1. Create a `pod` named `print-envars-greeting`.
2. Configure spec as, the container name should be `print-env-container` and use `bash` image.
3. Create three environment variables:
  a. `GREETING` and its value should be `Welcome to`
  b. `COMPANY` and its value should be `Nautilus`
  c. `GROUP` and its value should be `Group`
4. Use command `["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']` (please use this exact command), also set its `restartPolicy` policy to `Never` to avoid crash loop back.
5. You can check the output using `kubectl logs -f print-envars-greeting` command.

---

Check the existing resources:
```bash
$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   24m
```

Create the print-envars-greeting.yaml, apply and verify:
```bash
$ cat >print-envars-greeting.yaml
apiVersion: v1
kind: Pod
metadata:
  name: print-envars-greeting
spec:
  containers:
  - name: print-env-container
    image: bash
    command: ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']
    env:
    - name: GREETING
      value: "Welcome to"
    - name: COMPANY
      value: "Nautilus"
    - name: GROUP
      value: "Group"
  restartPolicy: Never

$ kubectl apply -f print-envars-greeting.yaml 
pod/print-envars-greeting created

$ kubectl get pods
NAME                    READY   STATUS      RESTARTS   AGE
print-envars-greeting   0/1     Completed   0          21s
```

Check the output:
```bash
$ kubectl logs -f print-envars-greeting
Welcome to Nautilus Group
```
✅

## 2.5 Rolling Updates And Rolling Back Deployments in Kubernetes

There is a production deployment planned for next week. The Nautilus DevOps team wants to test the deployment update and rollback on Dev environment first so that they can identify the risks in advance. Below you can find more details about the plan they want to execute.

Create a namespace `nautilus`. Create a deployment called `httpd-deploy` under this new namespace, It should have one container called `httpd`, use `httpd:2.4.28` image and `4` replicas. The deployment should use `RollingUpdate` strategy with `maxSurge=1`, and `maxUnavailable=2`. Also create a `NodePort` type service named `httpd-service` and expose the deployment on `nodePort: 30008`.

Now upgrade the deployment to version `httpd:2.4.43` using a rolling update.

Finally, once all pods are updated undo the recent update and roll back to the previous/original version.

`Note:` Please make sure you only use the specified image(s) for this deployment and as per the sequence mentioned in the task description. If you mistakenly use a wrong image and fix it later, that will also distort the revision history which can eventually fail this task.

---

Check the existing resources:
```bash
$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   64m
```

Create the namespace nautilus:
```bash
$ kubectl create namespace nautilus
namespace/nautilus created
```

Create the httpd-deploy.yaml, apply and verify:
```bash
$ cat > httpd-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deploy
  namespace: nautilus
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 2
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd:2.4.28

$ kubectl apply -f httpd-deploy.yaml 
deployment.apps/httpd-deploy created

$ kubectl get deployment -n nautilus
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
httpd-deploy   4/4     4            4           27s

$ kubectl get pods -n nautilus
NAME                           READY   STATUS    RESTARTS   AGE
httpd-deploy-7b45d54dc-4njsr   1/1     Running   0          39s
httpd-deploy-7b45d54dc-556v6   1/1     Running   0          39s
httpd-deploy-7b45d54dc-57vdn   1/1     Running   0          39s
httpd-deploy-7b45d54dc-nkwgs   1/1     Running   0          39s
```

Create the httpd-service.yaml, apply and verify:
```bash
$ cat httpd-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: httpd-service
  namespace: nautilus
spec:
  type: NodePort
  selector:
    app: httpd
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30008

$ kubectl apply -f httpd-service.yaml 
service/httpd-service created

$ kubectl get services -n nautilus
NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
httpd-service   NodePort   10.96.235.73   <none>        80:30008/TCP   18s
```

Perform rolling update and verify:
```bash
$ kubectl set image deployment httpd-deploy httpd=httpd:2.4.43 --namespace nautilus --record=true
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/httpd-deploy image updated

$ kubectl describe -n nautilus deployment/httpd-deploy | head -16
Name:                   httpd-deploy
Namespace:              nautilus
CreationTimestamp:      Tue, 13 Feb 2024 10:23:30 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment httpd-deploy httpd=httpd:2.4.43 --namespace=nautilus --record=true
Selector:               app=httpd
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  2 max unavailable, 1 max surge
Pod Template:
  Labels:  app=httpd
  Containers:
   httpd:
    Image:        httpd:2.4.43
```

Roll back to previous deployment and verify:
```bash
$ kubectl rollout undo deployment httpd-deploy -n nautilus 
deployment.apps/httpd-deploy rolled back

$ kubectl describe -n nautilus deployment/httpd-deploy | head -15
Name:                   httpd-deploy
Namespace:              nautilus
CreationTimestamp:      Tue, 13 Feb 2024 10:23:30 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=httpd
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  2 max unavailable, 1 max surge
Pod Template:
  Labels:  app=httpd
  Containers:
   httpd:
    Image:        httpd:2.4.28
```

![5](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/8c1a48e1-7fc7-4fb6-ab3d-ae6e97c7ce08)

✅

## 2.6 Deploy Jenkins on Kubernetes

The Nautilus DevOps team is planning to set up a Jenkins CI server to create/manage some deployment pipelines for some of the projects. They want to set up the Jenkins server on Kubernetes cluster. Below you can find more details about the task:

1. Create a namespace `jenkins`
2. Create a Service for jenkins deployment. Service name should be `jenkins-service` under `jenkins` namespace, type should be `NodePort`, nodePort should be `30008`
3. Create a Jenkins Deployment under jenkins namespace, It should be name as `jenkins-deployment`, labels app should be `jenkins`, container name should be `jenkins-container`, use `jenkins/jenkins` image, containerPort should be `8080` and replicas count should be `1`.

Make sure to wait for the pods to be in `running` state and make sure you are able to access the Jenkins login screen in the browser before hitting the Check button.

---

Create the namepsace jenkins:
```bash
$ kubectl create namespace jenkins
namespace/jenkins created
```

Create the deployment and service:
```bash
$ cat >jenkins-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: jenkins-service
  namespace: jenkins
spec:
  selector:
    app: jenkins
  type: NodePort
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30008

$ cat >jenkins-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deployment
  namespace: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
        - name: jenkins-container
          image: jenkins/jenkins
          ports:
            - containerPort: 8080
```

Apply:
```bash
$ kubectl apply -f jenkins-deployment.yaml 
deployment.apps/jenkins-deployment created

$ kubectl apply -f jenkins-service.yaml 
service/jenkins-service created
```

Check resources:
```bash
$ kubectl get all -n jenkins
NAME                                      READY   STATUS    RESTARTS   AGE
pod/jenkins-deployment-667887d68c-fqv9m   1/1     Running   0          27s

NAME                      TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/jenkins-service   NodePort   10.96.140.44   <none>        8080:30008/TCP   13s

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/jenkins-deployment   1/1     1            1           27s

NAME                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/jenkins-deployment-667887d68c   1         1         1       27s
```

![6.1](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/121b71e6-110e-42c4-812a-800b22a79c1e)

Find the password:
```bash
$ kubectl exec -it -n jenkins pod/jenkins-deployment-667887d68c-fqv9m -- cat /var/jenkins_home/secrets/initialAdminPassword
658976f91e29436d8f5587413209bda9
```

![6.2](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/e902bf4f-a076-4874-998e-56af0e9c8e31)

✅

## 2.7 Deploy Grafana on Kubernetes Cluster

The Nautilus DevOps teams is planning to set up a Grafana tool to collect and analyze analytics from some applications. They are planning to deploy it on Kubernetes cluster. Below you can find more details.

1. Create a deployment named `grafana-deployment-nautilus` using any `grafana` image for Grafana app. Set other parameters as per your choice.
2. Create `NodePort` type service with nodePort `32000` to expose the app.

You need not to make any configuration changes inside the Grafana app once deployed, just make sure you are able to access the Grafana login page.

---

Create the grafana-deployment.yaml, apply and verify:
```bash
$ cat >grafana-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-deployment-nautilus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000

$ kubectl apply -f grafana-deployment.yaml 
deployment.apps/grafana-deployment-nautilus created

$ kubectl get pods
NAME                                          READY   STATUS    RESTARTS   AGE
grafana-deployment-nautilus-77648df4c-5xz6p   1/1     Running   0          39s
```

Create the grafana-service.yaml, apply and verify:
```bash
$ cat >grafana-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana-service
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 32000

$ kubectl apply -f grafana-service.yaml 
service/grafana-service created

$ kubectl get services | head -2
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
grafana-service   NodePort    10.96.98.219   <none>        3000:32000/TCP   55s
```

![7](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/66fdb281-25e1-4eb7-bd4c-441bdf63f338)

✅

## 2.8 Deploy Tomcat App on Kubernetes

A new java-based application is ready to be deployed on a Kubernetes cluster. The development team had a meeting with the DevOps team to share the requirements and application scope. The team is ready to setup an application stack for it under their existing cluster. Below you can find the details for this:

1. Create a namespace named `tomcat-namespace-devops`.
2. Create a deployment for tomcat app which should be named as `tomcat-deployment-devops` under the same namespace you created. Replica count should be `1`, the container should be named as `tomcat-container-devops`, its image should be `gcr.io/kodekloud/centos-ssh-enabled:tomcat` and its container port should be `8080`.
3. Create a service for tomcat app which should be named as `tomcat-service-devops` under the same namespace you created. Service type should be `NodePort` and nodePort should be `32227`.

Before clicking on Check button please make sure the application is up and running.

You can use any labels as per your choice.

---

Create the namespace:
```bash
$ kubectl create namespace tomcat-namespace-devops
namespace/tomcat-namespace-devops created
```

Create the tomcat-deployment-devops.yaml, apply and verify:
```bash
$ cat >tomcat-deployment-devops.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment-devops
  namespace: tomcat-namespace-devops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat-app-devops
  template:
    metadata:
      labels:
        app: tomcat-app-devops
    spec:
      containers:
      - name: tomcat-container-devops
        image: gcr.io/kodekloud/centos-ssh-enabled:tomcat
        ports:
        - containerPort: 8080

$ kubectl apply -f tomcat-deployment-devops.yaml 
deployment.apps/tomcat-deployment-devops created

$ kubectl get pods -n tomcat-namespace-devops
NAME                                        READY   STATUS    RESTARTS   AGE
tomcat-deployment-devops-78d4b74dc9-4p5ft   1/1     Running   0          46s
```

Create the tomcat-service-devops.yaml, apply and verify:
```bash
$ cat >tomcat-service-devops.yaml
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service-devops
  namespace: tomcat-namespace-devops
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 32227
  selector:
    app: tomcat-app-devops

$ kubectl apply -f tomcat-service-devops.yaml 
service/tomcat-service-devops created

$ kubectl get services -n tomcat-namespace-devops
NAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
tomcat-service-devops   NodePort   10.96.134.212   <none>        8080:32227/TCP   39s
```

![8](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/23026b73-dec0-4212-afe4-079a6b94693c)

✅

## 2.9 Deploy Node App on Kubernetes

The Nautilus development team has completed development of one of the node applications, which they are planning to deploy on a Kubernetes cluster. They recently had a meeting with the DevOps team to share their requirements. Based on that, the DevOps team has listed out the exact requirements to deploy the app. Find below more details:

1. Create a deployment using `gcr.io/kodekloud/centos-ssh-enabled:node` image, replica count must be `2`.
2. Create a service to expose this app, the service type must be `NodePort`, targetPort must be `8080` and nodePort should be `30012`.
3. Make sure all the pods are in `Running` state after the deployment.
4. You can check the application by clicking on `NodeApp` button on top bar.

`You can use any labels as per your choice.`

---

Create the deployment.yaml, apply and verify:
```bash
$ cat >deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: centos-ssh-enabled-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: centos-ssh-enabled
  template:
    metadata:
      labels:
        app: centos-ssh-enabled
    spec:
      containers:
      - name: centos-ssh-enabled-container
        image: gcr.io/kodekloud/centos-ssh-enabled:node
        ports:
        - containerPort: 22

$ kubectl apply -f deployment.yaml

$ kubectl get pods
NAME                                             READY   STATUS    RESTARTS   AGE
centos-ssh-enabled-deployment-6965447b45-b5fdj   1/1     Running   0          80s
centos-ssh-enabled-deployment-6965447b45-bz86d   1/1     Running   0          80s
```

Create the service.yaml, apply and verify:
```bash
$ cat >service.yaml
apiVersion: v1
kind: Service
metadata:
  name: centos-ssh-enabled-service
spec:
  type: NodePort
  selector:
    app: centos-ssh-enabled
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30012

$ kubectl apply -f service.yaml 
service/centos-ssh-enabled-service created

$ kubectl get svc
NAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
centos-ssh-enabled-service   NodePort    10.96.152.234   <none>        8080:30012/TCP   37s
kubernetes                   ClusterIP   10.96.0.1       <none>        443/TCP          11m
```

![9](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/1e9f7550-01b7-454f-bb37-f6ed8408d6e7)

✅

## 2.10 Troubleshoot Deployment issues in Kubernetes

Last week, the Nautilus DevOps team deployed a redis app on Kubernetes cluster, which was working fine so far. This morning one of the team members was making some changes in this existing setup, but he made some mistakes and the app went down. We need to fix this as soon as possible. Please take a look.

The deployment name is `redis-deployment`. The pods are not in running state right now, so please look into the issue and fix the same.

---

Check the resources:
```bash
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   0/1     1            0           119s

$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
redis-deployment-54cdf4f76d-fzgbx   0/1     ContainerCreating   0          24m
```

Check the events for the deployment:
```bash
$ kubectl describe pod redis-deployment-54cdf4f76d-fzgbx | grep -A 10 Events
Events:
  Type     Reason       Age                   From               Message
  ----     ------       ----                  ----               -------
  Normal   Scheduled    27m                   default-scheduler  Successfully assigned default/redis-deployment-54cdf4f76d-fzgbx to kodekloud-control-plane
  Warning  FailedMount  5m22s (x10 over 25m)  kubelet            Unable to attach or mount volumes: unmounted volumes=[config], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition
  Warning  FailedMount  78s (x21 over 27m)    kubelet            MountVolume.SetUp failed for volume "config" : configmap "redis-conig" not found
```

Check the configmaps:
```bash
$ kubectl get configmaps
NAME               DATA   AGE
kube-root-ca.crt   1      72m
redis-config       2      39m
```


We see the typo in the name of config map which we need to fix:
```bash
$ kubectl edit deployment redis-deployment
deployment.apps/redis-deployment edited

          name: redis-conig
...
          name: redis-config

```

Check the pods:
```bash
$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
redis-deployment-54cdf4f76d-fzgbx   0/1     ContainerCreating   0          50m
redis-deployment-5bcd4c7d64-s46fs   0/1     ErrImagePull        0          64s
```

We see new error for which we can find more details in Events:
```bash 
$ kubectl describe pod redis-deployment-5bcd4c7d64-s46fs | grep -A 10 Events
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  2m46s                default-scheduler  Successfully assigned default/redis-deployment-5bcd4c7d64-s46fs to kodekloud-control-plane
  Normal   Pulling    73s (x4 over 2m46s)  kubelet            Pulling image "redis:alpin"
  Warning  Failed     72s (x4 over 2m45s)  kubelet            Failed to pull image "redis:alpin": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/redis:alpin": failed to resolve reference "docker.io/library/redis:alpin": docker.io/library/redis:alpin: not found
  Warning  Failed     72s (x4 over 2m45s)  kubelet            Error: ErrImagePull
  Warning  Failed     59s (x6 over 2m44s)  kubelet            Error: ImagePullBackOff
  Normal   BackOff    44s (x7 over 2m44s)  kubelet            Back-off pulling image "redis:alpin"
```

Image tag is not correct and we need to change it:
```bash
$ kubectl edit deployment redis-deployment
deployment.apps/redis-deployment edited

      - image: redis:alpin
...
      - image: redis:alpine
```

Verify:
```bash
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   1/1     1            1           53m

$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
redis-deployment-7c8d4f6ddf-lhxsr   1/1     Running   0          80s
```
✅

## 2.11 Fix issue with LAMP Environment in Kubernetes

One of the DevOps team member was trying to install a WordPress website on a LAMP stack which is essentially deployed on Kubernetes cluster. It was working well and we could see the installation page a few hours ago. However something is messed up with the stack now due to a website went down. Please look into the issue and fix it:

FYI, deployment name is `lamp-wp` and its using a service named `lamp-service`. The Apache is using http default port and nodeport is `30008`. From the application logs it has been identified that application is facing some issues while connecting to the database in addition to other issues. Additionally, there are some environment variables associated with the pods like `MYSQL_ROOT_PASSWORD, MYSQL_DATABASE,  MYSQL_USER, MYSQL_PASSWORD, MYSQL_HOST`.

Also do not try to delete/modify any other existing components like deployment name, service name, types, labels etc.

---

Check all existing resources in default namespace:
```bash
$ kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/lamp-wp-56c7c454fc-78r6n   2/2     Running   0          48s

NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP        32m
service/lamp-service    NodePort    10.96.224.213   <none>        80:30009/TCP   48s
service/mysql-service   ClusterIP   10.96.108.127   <none>        3306/TCP       48s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/lamp-wp   1/1     1            1           48s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/lamp-wp-56c7c454fc   1         1         1       48s
```

Port for service/lamp-service is 30009 which is incorrect and we need to change it to 30008:
```bash
$ kubectl edit service/lamp-service
service/lamp-service edited

  - nodePort: 30009
...
  - nodePort: 30008
```

Verify:
```bash
$ kubectl get services
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP        33m
lamp-service    NodePort    10.96.224.213   <none>        80:30008/TCP   2m43s
mysql-service   ClusterIP   10.96.108.127   <none>        3306/TCP       2m43s
```

Let's see more details about the pod:
```bash
$ kubectl describe pod lamp-wp-56c7c454fc-78r6n
Name:             lamp-wp-56c7c454fc-78r6n
Namespace:        default
Priority:         0
Service Account:  default
Node:             kodekloud-control-plane/172.17.0.2
Start Time:       Mon, 19 Feb 2024 19:00:46 +0000
Labels:           app=lamp
                  pod-template-hash=56c7c454fc
                  tier=frontend
Annotations:      <none>
Status:           Running
IP:               10.244.0.5
IPs:
  IP:           10.244.0.5
Controlled By:  ReplicaSet/lamp-wp-56c7c454fc
Containers:
  httpd-php-container:
    Container ID:   containerd://49d2f7de78b80314c8e7eeee968aa82932684ba40daa866766d00c4a9d0ab181
    Image:          webdevops/php-apache:alpine-3-php7
    Image ID:       docker.io/webdevops/php-apache@sha256:bb68c986d4947d4cb49e2753a268e33ad3d69df29c8e9a7728090f4738d5bdb9
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 19 Feb 2024 19:00:57 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'password' in secret 'mysql-root-pass'>  Optional: false
      MYSQL_DATABASE:       <set to the key 'database' in secret 'mysql-db-url'>     Optional: false
      MYSQL_USER:           <set to the key 'username' in secret 'mysql-user-pass'>  Optional: false
      MYSQL_PASSWORD:       <set to the key 'password' in secret 'mysql-user-pass'>  Optional: false
      MYSQL_HOST:           <set to the key 'host' in secret 'mysql-host'>           Optional: false
    Mounts:
      /opt/docker/etc/php/php.ini from php-config-volume (rw,path="php.ini")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6sv4t (ro)
  mysql-container:
    Container ID:   containerd://591a6f61f57c2ae7bb4a73b31ce38a81752cc748b5790bdb63f41c3ad77f9681
    Image:          mysql:5.6
    Image ID:       docker.io/library/mysql@sha256:20575ecebe6216036d25dab5903808211f1e9ba63dc7825ac20cb975e34cfcae
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 19 Feb 2024 19:01:13 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'password' in secret 'mysql-root-pass'>  Optional: false
      MYSQL_DATABASE:       <set to the key 'database' in secret 'mysql-db-url'>     Optional: false
      MYSQL_USER:           <set to the key 'username' in secret 'mysql-user-pass'>  Optional: false
      MYSQL_PASSWORD:       <set to the key 'password' in secret 'mysql-user-pass'>  Optional: false
      MYSQL_HOST:           <set to the key 'host' in secret 'mysql-host'>           Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6sv4t (ro)
...
```

Execute into the pod and check the configuration:
```bash
$ kubectl exec -it lamp-wp-56c7c454fc-78r6n -c httpd-php-container -- sh
/ # vi app/index.php 
...
<?php
$dbname = $_ENV['MYSQL_DATABASE'];
$dbuser = $_ENV['MYSQL_USER'];
$dbpass = $_ENV[''MYSQL_PASSWORD""];
$dbhost = $_ENV['MYSQL-HOST'];


$connect = mysqli_connect($dbhost, $dbuser, $dbpass) or die("Unable to Connect to '$dbhost'");

$test_query = "SHOW TABLES FROM $dbname";
$result = mysqli_query($test_query);

if ($result->connect_error) {
   die("Connection failed: " . $conn->connect_error);
}
  echo "Connected successfully";
...
```

Fix the typos:
```bash
$dbpass = $_ENV['MYSQL_PASSWORD'];
$dbhost = $_ENV['MYSQL_HOST'];
```

Restart the service and verify:
```bash
$ service apache2 restart
apache:apached: stopped
apache:apached: started

$ curl http://localhost:80
Connected successfully
```

![1](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/7453952d-647f-4780-a42e-44ae9f2832c3)

✅

![k8s-level-2](https://github.com/adinpilavdzija/kodekloud-engineer/assets/65655945/688249e0-acbe-4497-8873-63d3f5e6ca53)